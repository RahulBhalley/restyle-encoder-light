{"metadata":{"accelerator":"GPU","colab":{"name":"inference_playground.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://kaggle.com/kernels/welcome?src=https://github.com/yuval-alaluf/restyle-encoder/blob/main/notebooks/inference_playground.ipynb\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a><a href=\"https://colab.research.google.com/github/yuval-alaluf/restyle-encoder/blob/main/notebooks/inference_playground.ipynb\"><img align=\"left\" title=\"Open in Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"></a>","metadata":{}},{"cell_type":"code","source":"import os\n# os.chdir('/content')\nCODE_DIR = 'restyle-encoder-light'","metadata":{"id":"Uuviq3qQkUFy","execution":{"iopub.status.busy":"2022-03-09T12:19:11.474614Z","iopub.execute_input":"2022-03-09T12:19:11.475379Z","iopub.status.idle":"2022-03-09T12:19:11.509974Z","shell.execute_reply.started":"2022-03-09T12:19:11.475245Z","shell.execute_reply":"2022-03-09T12:19:11.509088Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/RahulBhalley/restyle-encoder-light.git $CODE_DIR","metadata":{"id":"QQ6XEmlHlXbk","execution":{"iopub.status.busy":"2022-03-09T12:19:28.970184Z","iopub.execute_input":"2022-03-09T12:19:28.970441Z","iopub.status.idle":"2022-03-09T12:19:31.449008Z","shell.execute_reply.started":"2022-03-09T12:19:28.970413Z","shell.execute_reply":"2022-03-09T12:19:31.447958Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n!sudo unzip ninja-linux.zip -d /usr/local/bin/\n!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force","metadata":{"id":"JaRUFuVHkzye","execution":{"iopub.status.busy":"2022-03-09T12:19:33.616154Z","iopub.execute_input":"2022-03-09T12:19:33.616437Z","iopub.status.idle":"2022-03-09T12:19:36.720678Z","shell.execute_reply.started":"2022-03-09T12:19:33.616408Z","shell.execute_reply":"2022-03-09T12:19:36.719497Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"os.chdir(f'./{CODE_DIR}')","metadata":{"id":"23baccYQlU9E","execution":{"iopub.status.busy":"2022-03-09T12:19:47.688642Z","iopub.execute_input":"2022-03-09T12:19:47.688933Z","iopub.status.idle":"2022-03-09T12:19:47.695644Z","shell.execute_reply.started":"2022-03-09T12:19:47.688902Z","shell.execute_reply":"2022-03-09T12:19:47.694078Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from argparse import Namespace\nimport time\nimport os\nimport sys\nimport pprint\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torchvision.transforms as transforms\n\nsys.path.append(\".\")\nsys.path.append(\"..\")\n\nfrom utils.common import tensor2im\nfrom models.psp import pSp\nfrom models.e4e import e4e\n\n%load_ext autoreload\n%autoreload 2","metadata":{"id":"d13v7In0kTJn","execution":{"iopub.status.busy":"2022-03-09T12:24:07.876552Z","iopub.execute_input":"2022-03-09T12:24:07.877027Z","iopub.status.idle":"2022-03-09T12:24:07.924667Z","shell.execute_reply.started":"2022-03-09T12:24:07.876979Z","shell.execute_reply":"2022-03-09T12:24:07.923629Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Step 1: Select Experiment Type\nSelect which experiment you wish to perform inference on:","metadata":{"id":"HRjtz6uLkTJs"}},{"cell_type":"code","source":"#@title Select which experiment you wish to perform inference on: { run: \"auto\" }\nexperiment_type = 'horse_encode' #@param ['ffhq_encode', 'cars_encode', 'church_encode', 'horse_encode', 'afhq_wild_encode', 'toonify']","metadata":{"id":"XESWAO65kTJt","execution":{"iopub.status.busy":"2022-03-09T12:24:14.437226Z","iopub.execute_input":"2022-03-09T12:24:14.437541Z","iopub.status.idle":"2022-03-09T12:24:14.479277Z","shell.execute_reply.started":"2022-03-09T12:24:14.437509Z","shell.execute_reply":"2022-03-09T12:24:14.478150Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Step 2: Prepare to Download Pretrained Models \nAs part of this repository, we provide pretrained models for each of the above experiments. Here, we'll create the download command needed for downloading the desired model.\n\nNote: in this notebook, we'll be using ReStyle applied over pSp for all domains except for the horses domain where we'll be using e4e. This is done since e4e is generally able to generate more realistic reconstructions on this domain. ","metadata":{"id":"4etDz82xkTJz"}},{"cell_type":"code","source":"def get_download_model_command(file_id, file_name):\n    \"\"\" Get wget download command for downloading the desired model and save to directory ../pretrained_models. \"\"\"\n    current_directory = os.getcwd()\n    save_path = os.path.join(os.path.dirname(current_directory), CODE_DIR, \"pretrained_models\")\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n    url = r\"\"\"wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id={FILE_ID}' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id={FILE_ID}\" -O {SAVE_PATH}/{FILE_NAME} && rm -rf /tmp/cookies.txt\"\"\".format(FILE_ID=file_id, FILE_NAME=file_name, SAVE_PATH=save_path)\n    return url    ","metadata":{"id":"KSnjlBZOkTJ0","execution":{"iopub.status.busy":"2022-03-09T12:24:18.111698Z","iopub.execute_input":"2022-03-09T12:24:18.111985Z","iopub.status.idle":"2022-03-09T12:24:18.155194Z","shell.execute_reply.started":"2022-03-09T12:24:18.111955Z","shell.execute_reply":"2022-03-09T12:24:18.154122Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"MODEL_PATHS = {\n    \"ffhq_encode\": {\"id\": \"1sw6I2lRIB0MpuJkpc8F5BJiSZrc0hjfE\", \"name\": \"restyle_psp_ffhq_encode.pt\"},\n    \"cars_encode\": {\"id\": \"1zJHqHRQ8NOnVohVVCGbeYMMr6PDhRpPR\", \"name\": \"restyle_psp_cars_encode.pt\"},\n    \"church_encode\": {\"id\": \"1bcxx7mw-1z7dzbJI_z7oGpWG1oQAvMaD\", \"name\": \"restyle_psp_church_encode.pt\"},\n    \"horse_encode\": {\"id\": \"19_sUpTYtJmhSAolKLm3VgI-ptYqd-hgY\", \"name\": \"restyle_e4e_horse_encode.pt\"},\n    \"afhq_wild_encode\": {\"id\": \"1GyFXVTNDUw3IIGHmGS71ChhJ1Rmslhk7\", \"name\": \"restyle_psp_afhq_wild_encode.pt\"},\n    \"toonify\": {\"id\": \"1GtudVDig59d4HJ_8bGEniz5huaTSGO_0\", \"name\": \"restyle_psp_toonify.pt\"}\n}\n\npath = MODEL_PATHS[experiment_type]\ndownload_command = get_download_model_command(file_id=path[\"id\"], file_name=path[\"name\"]) ","metadata":{"id":"m4sjldFMkTJ5","execution":{"iopub.status.busy":"2022-03-09T12:24:19.373040Z","iopub.execute_input":"2022-03-09T12:24:19.373442Z","iopub.status.idle":"2022-03-09T12:24:19.418848Z","shell.execute_reply.started":"2022-03-09T12:24:19.373412Z","shell.execute_reply":"2022-03-09T12:24:19.417749Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Step 3: Define Inference Parameters","metadata":{"id":"9Tozsg81kTKA"}},{"cell_type":"markdown","source":"Below we have a dictionary defining parameters such as the path to the pretrained model to use and the path to the image to perform inference on.  \nWhile we provide default values to run this script, feel free to change as needed.","metadata":{"id":"XIhyc7RqkTKB"}},{"cell_type":"code","source":"EXPERIMENT_DATA_ARGS = {\n    \"ffhq_encode\": {\n        \"model_path\": \"pretrained_models/restyle_psp_ffhq_encode.pt\",\n        \"image_path\": \"notebooks/images/face_img.jpg\",\n        \"transform\": transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n    },\n    \"cars_encode\": {\n        \"model_path\": \"pretrained_models/restyle_psp_cars_encode.pt\",\n        \"image_path\": \"notebooks/images/car_img.jpg\",\n        \"transform\": transforms.Compose([\n            transforms.Resize((192, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n    },\n    \"church_encode\": {\n        \"model_path\": \"pretrained_models/restyle_psp_church_encode.pt\",\n        \"image_path\": \"notebooks/images/church_img.jpg\",\n        \"transform\": transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n    },\n    \"horse_encode\": {\n        \"model_path\": \"pretrained_models/restyle_e4e_horse_encode.pt\",\n        \"image_path\": \"notebooks/images/horse_img.jpg\",\n        \"transform\": transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n    },\n    \"afhq_wild_encode\": {\n        \"model_path\": \"pretrained_models/restyle_psp_afhq_wild_encode.pt\",\n        \"image_path\": \"notebooks/images/afhq_wild_img.jpg\",\n        \"transform\": transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n    },\n    \"toonify\": {\n        \"model_path\": \"pretrained_models/restyle_psp_toonify.pt\",\n        \"image_path\": \"notebooks/images/toonify_img.jpg\",\n        \"transform\": transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n    },\n}","metadata":{"id":"2kE5y1-skTKC","execution":{"iopub.status.busy":"2022-03-09T12:24:21.051158Z","iopub.execute_input":"2022-03-09T12:24:21.051810Z","iopub.status.idle":"2022-03-09T12:24:21.101908Z","shell.execute_reply.started":"2022-03-09T12:24:21.051776Z","shell.execute_reply":"2022-03-09T12:24:21.100809Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS[experiment_type]","metadata":{"id":"IzUHoD9ukTKG","execution":{"iopub.status.busy":"2022-03-09T12:24:21.774271Z","iopub.execute_input":"2022-03-09T12:24:21.774533Z","iopub.status.idle":"2022-03-09T12:24:21.816359Z","shell.execute_reply.started":"2022-03-09T12:24:21.774504Z","shell.execute_reply":"2022-03-09T12:24:21.815304Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"To reduce the number of requests to fetch the model, we'll check if the model was previously downloaded and saved before downloading the model.  \nWe'll download the model for the selected experiment and save it to the folder `../pretrained_models`.\n\nWe also need to verify that the model was downloaded correctly. All of our models should weigh approximately 800MB - 1GB.  \nNote that if the file weighs several KBs, you most likely encounter a \"quota exceeded\" error from Google Drive. In that case, you should try downloading the model again after a few hours.","metadata":{}},{"cell_type":"code","source":"if not os.path.exists(EXPERIMENT_ARGS['model_path']) or os.path.getsize(EXPERIMENT_ARGS['model_path']) < 1000000:\n    print(f'Downloading ReStyle model for {experiment_type}...')\n    os.system(f\"wget {download_command}\")\n    # if google drive receives too many requests, we'll reach the quota limit and be unable to download the model\n    if os.path.getsize(EXPERIMENT_ARGS['model_path']) < 1000000:\n        raise ValueError(\"Pretrained model was unable to be downloaded correctly!\")\n    else:\n        print('Done.')\nelse:\n    print(f'ReStyle model for {experiment_type} already exists!')","metadata":{"id":"jQ31J_m7kTJ8","scrolled":true,"execution":{"iopub.status.busy":"2022-03-09T12:24:24.190395Z","iopub.execute_input":"2022-03-09T12:24:24.190870Z","iopub.status.idle":"2022-03-09T12:24:39.463803Z","shell.execute_reply.started":"2022-03-09T12:24:24.190819Z","shell.execute_reply":"2022-03-09T12:24:39.462787Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Step 4: Load Pretrained Model\nWe assume that you have downloaded all relevant models and placed them in the directory defined by the above dictionary.","metadata":{"id":"TAWrUehTkTKJ"}},{"cell_type":"code","source":"model_path = EXPERIMENT_ARGS['model_path']\nckpt = torch.load(model_path, map_location='cpu')","metadata":{"id":"1t-AOhP1kTKJ","execution":{"iopub.status.busy":"2022-03-09T12:45:53.655808Z","iopub.execute_input":"2022-03-09T12:45:53.656092Z","iopub.status.idle":"2022-03-09T12:45:54.127647Z","shell.execute_reply.started":"2022-03-09T12:45:53.656063Z","shell.execute_reply":"2022-03-09T12:45:54.126724Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"opts = ckpt['opts']\npprint.pprint(opts)","metadata":{"id":"2UBwJ3dJkTKM","execution":{"iopub.status.busy":"2022-03-09T12:45:54.129496Z","iopub.execute_input":"2022-03-09T12:45:54.129899Z","iopub.status.idle":"2022-03-09T12:45:54.192174Z","shell.execute_reply.started":"2022-03-09T12:45:54.129854Z","shell.execute_reply":"2022-03-09T12:45:54.191303Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# update the training options\nopts['checkpoint_path'] = model_path\n# opts['encoder_type'] = 'ResNetProgressiveBackboneEncoderLight'\nopts['device'] = 'cpu'","metadata":{"id":"EMKhWoFKkTKS","execution":{"iopub.status.busy":"2022-03-09T12:46:06.157321Z","iopub.execute_input":"2022-03-09T12:46:06.157597Z","iopub.status.idle":"2022-03-09T12:46:06.199079Z","shell.execute_reply.started":"2022-03-09T12:46:06.157568Z","shell.execute_reply":"2022-03-09T12:46:06.197908Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"opts = Namespace(**opts)\nif experiment_type == 'horse_encode': \n    net = e4e(opts)\nelse:\n    net = pSp(opts)\n    \nnet.eval()\n# net.cuda()\nprint('Model successfully loaded!')","metadata":{"id":"6hccfNizkTKW","execution":{"iopub.status.busy":"2022-03-09T12:46:09.057216Z","iopub.execute_input":"2022-03-09T12:46:09.057620Z","iopub.status.idle":"2022-03-09T12:46:12.012149Z","shell.execute_reply.started":"2022-03-09T12:46:09.057586Z","shell.execute_reply":"2022-03-09T12:46:12.011002Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"type(net.encoder)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T12:46:13.229893Z","iopub.execute_input":"2022-03-09T12:46:13.230184Z","iopub.status.idle":"2022-03-09T12:46:13.274332Z","shell.execute_reply.started":"2022-03-09T12:46:13.230140Z","shell.execute_reply":"2022-03-09T12:46:13.273281Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"type(net.decoder)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T12:46:16.144620Z","iopub.execute_input":"2022-03-09T12:46:16.145256Z","iopub.status.idle":"2022-03-09T12:46:16.188522Z","shell.execute_reply.started":"2022-03-09T12:46:16.145225Z","shell.execute_reply":"2022-03-09T12:46:16.187116Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"x = torch.randn(1, 6, 1024, 1024)\nencoder_trace = torch.jit.trace(net.encoder, x)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T12:46:36.780813Z","iopub.execute_input":"2022-03-09T12:46:36.781145Z","iopub.status.idle":"2022-03-09T12:47:42.426670Z","shell.execute_reply.started":"2022-03-09T12:46:36.781091Z","shell.execute_reply":"2022-03-09T12:47:42.425496Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"torch.jit.save(encoder_trace, 'ResNetProgressiveBackboneEncoder_trace.pt')","metadata":{"execution":{"iopub.status.busy":"2022-03-09T12:48:10.891750Z","iopub.execute_input":"2022-03-09T12:48:10.892034Z","iopub.status.idle":"2022-03-09T12:48:11.763571Z","shell.execute_reply.started":"2022-03-09T12:48:10.891989Z","shell.execute_reply":"2022-03-09T12:48:11.762374Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"## Step 5: Visualize Input","metadata":{"id":"4weLFoPbkTKZ"}},{"cell_type":"code","source":"image_path = EXPERIMENT_DATA_ARGS[experiment_type][\"image_path\"]\noriginal_image = Image.open(image_path).convert(\"RGB\")","metadata":{"id":"r2H9zFLJkTKa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if experiment_type == 'cars_encode':\n    original_image = original_image.resize((192, 256))\nelse:\n    original_image = original_image.resize((256, 256))","metadata":{"id":"-lbLKtl-kTKc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_image","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Align Image\n\nNote: in this notebook we'll run alignment on the input image when working on the human facial domain.","metadata":{"id":"o6oqf8JwzK0K"}},{"cell_type":"code","source":"def run_alignment(image_path):\n    import dlib\n    from scripts.align_faces_parallel import align_face\n    if not os.path.exists(\"shape_predictor_68_face_landmarks.dat\"):\n        print('Downloading files for aligning face image...')\n        os.system('wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2')\n        os.system('bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2')\n        print('Done.')\n    predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n    aligned_image = align_face(filepath=image_path, predictor=predictor) \n    print(\"Aligned image has shape: {}\".format(aligned_image.size))\n    return aligned_image ","metadata":{"id":"hJ9Ce1aYzmFF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if experiment_type in ['ffhq_encode', 'toonify']:\n    input_image = run_alignment(image_path)\nelse:\n    input_image = original_image","metadata":{"id":"aTZcKMdK8y77"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_image.resize((256, 256))","metadata":{"id":"hUBAfodh5PaM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 6: Perform Inference","metadata":{"id":"D0BmXzu1kTKg"}},{"cell_type":"code","source":"img_transforms = EXPERIMENT_ARGS['transform']\ntransformed_image = img_transforms(input_image)","metadata":{"id":"T3h3E7VLkTKg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before running inference, we need to generate the image corresponding to the average latent code. These will be used to initialize the iterative refinement process.","metadata":{"id":"_fNBlRU8OSDL"}},{"cell_type":"code","source":"def get_avg_image(net):\n    avg_image = net(net.latent_avg.unsqueeze(0),\n                    input_code=True,\n                    randomize_noise=False,\n                    return_latents=False,\n                    average_code=True)[0]\n    avg_image = avg_image.to('cuda').float().detach()\n    if experiment_type == \"cars_encode\":\n        avg_image = avg_image[:, 32:224, :]\n    return avg_image","metadata":{"id":"fmpzoODNOSDL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we'll run inference. By default, we'll run using 5 inference steps. You can change the parameter in the cell below.","metadata":{"id":"M5eWR2S4OSDM"}},{"cell_type":"code","source":"opts.n_iters_per_batch = 5\nopts.resize_outputs = False  # generate outputs at full resolution","metadata":{"id":"Ct_jm0obOSDM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from utils.inference_utils import run_on_batch\n\nwith torch.no_grad():\n    avg_image = get_avg_image(net)\n    tic = time.time()\n    result_batch, result_latents = run_on_batch(transformed_image.unsqueeze(0).cuda(), net, opts, avg_image)\n    toc = time.time()\n    print('Inference took {:.4f} seconds.'.format(toc - tic))","metadata":{"id":"Ls5zb0fRkTKs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize Result","metadata":{"id":"Nq0dkSz6kTKv"}},{"cell_type":"markdown","source":"We'll visualize the step-by-step outputs side by side.","metadata":{"id":"UVR03XT_kTK0"}},{"cell_type":"code","source":"if opts.dataset_type == \"cars_encode\":\n    resize_amount = (256, 192) if opts.resize_outputs else (512, 384)\nelse:\n    resize_amount = (256, 256) if opts.resize_outputs else (opts.output_size, opts.output_size)","metadata":{"id":"ca5BtxdUOSDN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_coupled_results(result_batch, transformed_image):\n    \"\"\"\n    Visualize output images from left to right (the input image is on the right)\n    \"\"\"\n    result_tensors = result_batch[0]  # there's one image in our batch\n    result_images = [tensor2im(result_tensors[iter_idx]) for iter_idx in range(opts.n_iters_per_batch)]\n    input_im = tensor2im(transformed_image)\n    res = np.array(result_images[0].resize(resize_amount))\n    for idx, result in enumerate(result_images[1:]):\n        res = np.concatenate([res, np.array(result.resize(resize_amount))], axis=1)\n    res = np.concatenate([res, input_im.resize(resize_amount)], axis=1)\n    res = Image.fromarray(res)\n    return res","metadata":{"id":"WdR51hOROSDN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that the step-by-step outputs are shown left-to-right with the original input on the right-hand side.","metadata":{"id":"uSDCvtTMOSDN"}},{"cell_type":"code","source":"res = get_coupled_results(result_batch, transformed_image)\nres","metadata":{"id":"lb3raAKFOSDN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save image \nres.save(f'./{experiment_type}_results.jpg')","metadata":{"id":"qaB7RN7cOSDN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoder Bootstrapping\n\nIn the paper, we introduce an encoder bootstrapping technique that can be used to solve the image toonification task by pairing an FFHQ-based encoder with a Toon-based encoder.  \n\nWe demonstrate this idea below.","metadata":{"id":"ISEMFxmekTK7"}},{"cell_type":"code","source":"# download the ffhq-based encoder if not previously downloaded\npath = MODEL_PATHS['ffhq_encode']\nEXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS['ffhq_encode']\nffhq_model_path = EXPERIMENT_ARGS['model_path']\ndownload_command = get_download_model_command(file_id=path[\"id\"], file_name=path[\"name\"]) \nif not os.path.exists(ffhq_model_path):\n    print('Downloading FFHQ ReStyle encoder...')\n    os.system(f\"wget {download_command}\")\n    print('Done.')\nelse:\n    print('FFHQ ReStyle encoder already exists!')","metadata":{"id":"Sv284Ox8OSDO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# download the toon-based encoder if not previously downloaded\npath = MODEL_PATHS['toonify']\nEXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS['toonify']\ntoonify_model_path = EXPERIMENT_ARGS['model_path']\ndownload_command = get_download_model_command(file_id=path[\"id\"], file_name=path[\"name\"]) \nif not os.path.exists(toonify_model_path):\n    print('Downloading Toonify ReStyle encoder...')\n    os.system(f\"wget {download_command}\")\n    print('Done.')\nelse:\n    print('Toonify ReStyle encoder already exists!')","metadata":{"id":"FKbAFK7_OSDO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load models \nckpt = torch.load(ffhq_model_path, map_location='cpu')\nopts = ckpt['opts']\nopts['checkpoint_path'] = ffhq_model_path\nopts = Namespace(**opts)\nnet1 = pSp(opts)\nnet1.eval()\nnet1.cuda()\nprint('FFHQ Model successfully loaded!')\n\nckpt = torch.load(toonify_model_path, map_location='cpu')\nopts = ckpt['opts']\nopts['checkpoint_path'] = toonify_model_path\nopts = Namespace(**opts)\nnet2 = pSp(opts)\nnet2.eval()\nnet2.cuda()\nprint('Toonify Model successfully loaded!')","metadata":{"id":"K3v0X3ZWkTK8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load image \nimage_path = EXPERIMENT_DATA_ARGS['toonify'][\"image_path\"]\noriginal_image = Image.open(image_path).convert(\"RGB\")","metadata":{"id":"XW-CJsuwOSDO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transform image\nimg_transforms = EXPERIMENT_ARGS['transform']\ntransformed_image = img_transforms(original_image)","metadata":{"id":"MmPWPODaOSDP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opts.n_iters_per_batch = 5\nopts.resize_outputs = False  # generate outputs at full resolution","metadata":{"id":"BiMjTyMzOSDP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scripts.encoder_bootstrapping_inference import run_on_batch\n\nwith torch.no_grad():\n    avg_image = get_avg_image(net1)\n    tic = time.time()\n    result_batch = run_on_batch(transformed_image.unsqueeze(0).cuda(), net1, net2, opts, avg_image)\n    toc = time.time()\n    print('Inference took {:.4f} seconds.'.format(toc - tic))","metadata":{"id":"o81i-MtOOSDQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again we'll visualize the results from left to right. Here, the leftmost image is the inverted FFHQ image that is used to initialize the toonify ReStyle encoder. The following images show iterative results outputted by the toonify model.\nFinally, the rightmost image is the original input image.","metadata":{"id":"1AGWnm9BOSDQ"}},{"cell_type":"code","source":"res = get_coupled_results(result_batch, transformed_image)\nres","metadata":{"id":"FX-_45rxOSDQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save image \nres.save(f'./encoder_bootstrapping_results.jpg')","metadata":{"id":"Pdk_QLRFOSDQ"},"execution_count":null,"outputs":[]}]}